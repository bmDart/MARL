This file contains the experiments for "Evolutionary multi-agent reinforcement learning in group social dilemmas" by Brian Mintz and Feng Fu, published as an editor's choice 2/12/2025 in Chaos: An Interdiscplinary Journal of Nonlinear Science. 

**Abstract**: Reinforcement learning (RL) is a powerful machine learning technique that has been successfully applied to a wide variety of problems. However, it can be unpredictable and produce suboptimal results in complicated learning environments. This is especially true when multiple agents learn simultaneously, which creates a complex system that is often analytically intractable. Our work considers the fundamental framework of Q-learning in public goods games, where RL individuals must work together to achieve a common goal. This setting allows us to study the tragedy of the commons and free-rider effects in artificial intelligence cooperation, an emerging field with potential to resolve challenging obstacles to the wider application of artificial intelligence. While this social dilemma has been mainly investigated through traditional and evolutionary game theory, our work connects these two approaches by studying agents with an intermediate level of intelligence. We consider the influence of learning parameters on cooperation levels in simulations and a limiting system of differential equations, as well as the effect of evolutionary pressures on exploration rate in both of these models. We find selection for higher and lower levels of exploration, as well as attracting values, and a condition that separates these in a restricted class of games. Our work enhances the theoretical understanding of recent techniques that combine evolutionary algorithms with Q-learning and extends our knowledge of the evolution of machine behavior in social dilemmas.


Motivation and gaps addressed:
1. Most of the MARL literature focuses on static groups, this evolutionary model incorporates dynamics groups, further destabilizing the learning environment. Similarly, many studies only consider a small number of agents, whereas this work considers larger groups, allowing for more complexity. 
2. This bridges the gap between traditional and evolutionary game theory by considering agents with intermediate complexity.

Research Question:
1. How does adding evolution change the dynamics, in particular, does it lead  to higher contribution? Can evolution solve this dilemma in learning agents?
2. How does the exploration rate evolve naturally? Are genetic algorithms a viable option for tuning this hyper-parameter for reinforcement learning in this application?
